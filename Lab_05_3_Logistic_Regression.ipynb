{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-05-3 Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGtEDMALqHUaTsY0xcR6g2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow/blob/master/study-note/Lab_05_3_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnfq-_6Z_GY4",
        "colab_type": "text"
      },
      "source": [
        "# Lab-05-3 Logistic Regression\n",
        "\n",
        "1. `logistic regression`의 `hypothesis`는 `Linear function`을 `sigmoid`에 인풋한 형태이다. <br />\n",
        "> 이것의 의미는 `logistic regression`이 2진 분류(?)에 해당되기에<br />\n",
        "즉, 참/거짓 둘중에 하나의 값으로 나와야한다는 이야기이다. <br />\n",
        "따라서 `sigmoid`처럼 한 쪽은 1에 수렴, 반대는  0으로 수렴하는 형태가 `Hypothesis`가 되어야한다.\n",
        "\n",
        "2. 이제 이전강의에서 나온 log를 사용한 `cost function`을 사용하여그 오차를 구해야한다.\n",
        "> 어쨋거나 `Hypothesis`가 정의되었으니, $h(x)$로 부터의 아웃풋이 있을것이다.<br /> \n",
        "우리는 그 아웃풋이 0 또는 1이 나와야한다.(참/거짓 중 하나여야한다.)<br />\n",
        "그 값이 0과 1로부터 오차가 얼마인지 구하는 것이다.\n",
        "\n",
        "3. 그 오차를 토대로 `optimization`과정을 거친다(`Gradient Descent`같은거 하란 말.)\n",
        "> 이 방식을 통해 오차를 구해서 `W(matrix of parameters)`를 `optimize`시키면<br />\n",
        "W는 단순히 데이터를 점으로 포함하는 그래프가 아닌 데이터를 참/거짓으로 양분하는<br />\n",
        "그래프의 형태가 되도록 업데이트 될 것이다.\n",
        "\n",
        "\n",
        "# 결론\n",
        "이러한 `Logistic regression`과정은 `neural network`에서 한 `neuron`을 업데이트 하는 과정에 해당된다.\n",
        "지금까지는 `binary classification`에 해당하는 과정이다.\n"
      ]
    }
  ]
}