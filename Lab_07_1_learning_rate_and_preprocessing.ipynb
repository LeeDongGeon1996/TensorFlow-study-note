{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-07-1-learning_rate_and_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZu5530uOf0pdx0eLySC/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow/blob/master/study-note/Lab_07_1_learning_rate_and_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOrxaE6jCnia",
        "colab_type": "text"
      },
      "source": [
        "# Lab-07-1-learning_rate_and_evaluation-eager\n",
        "###-------------------------------------------------------------------\n",
        "* learning rate는 어떻게 정해야 좋은가.\n",
        "* 학습을 시킬 데이터의 전처리 방법.\n",
        "\n",
        "###-------------------------------------------------------------------\n",
        "\n",
        "##learning rate는 어떻게 정해야 좋은가.\n",
        "* 데이터를 통해 모델을 생성해나갈때 `learning rate`는 매우 중요하다.\n",
        "* `learning rate`는 `hyper-parameter`이다.\n",
        "> `hyper-parameter`는 모델을 만들어 가기위한 설정값.\n",
        "* `learning rate`를 통해 기울기를 활용하여 얼마만큼 `weight`값을 조정할 것인가를 설정한다.\n",
        "* 그래프상에서 얼마나 하강할것인가를 결정하는 요소라는 것이다.\n",
        "> 즉, 너무 크게 설정하면 오히려 cost가 커질수도있고 (=`overshooting`)<br />\n",
        "반대로 너무 작게 설정하면 학습이 제대로 진행되지 않을 수 있다. (하강이 너무 느리게 진행된다.)\n",
        ">> * 일반적으로 많이 쓰는 `learning rete`는  0.01<br />\n",
        ">> * `Adam optimizer`의 경우에는 $3*10^{-4}$의 값이 가장 적절하다고함.\n",
        "\n",
        "## 그래서 어떻게 하면좋은가.\n",
        "* 우리가 좋은 `learning rate`를 구했다고하더라도 <br />\n",
        "학습하는 과정에서 `learning rate`를 적절히 조정하는 과정이 필요함.<br />\n",
        "> `learning rate decay`\n",
        "\n",
        "### Learning rate Dacay\n",
        "* 학습을 진행시키다보면 더 이상 cost가 줄어들지 않는 <br />\n",
        "즉, 학습이 더 이상 진행되지 않는 상황이 발생하는 경우가 있다.\n",
        "\n",
        "* 이 때, `learning rate decay`를 함으로써 더욱 cost 값을 최소화 시킬 수 있다.<br />\n",
        "다른 말로 더욱 `optimize`시킬 수 있다고 말할 수 있을듯.<br />\n",
        "\n",
        "> * Epoch마다 하는 기법\n",
        "* Exponential하게 감소하는 기법.\n",
        "* 다양한 `decay`기법들이 `tensorflow` 라이브러리에서 지원됨.\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "## 학습을 시킬 데이터의 전처리 방법.\n",
        "\n",
        "### 1. Feature Scaling\n",
        " * Standardization(표준화) : 평균값으로부터 평균거리를 측정. <br />\n",
        " 데이터가 평균거리상에서 어디에 있는지 표현.\n",
        " > $x_{new} = \\frac{x-평균}{표준편차}$\n",
        "\n",
        " * Normalization(정규화) : 데이터의 전체범위에서 데이터의 위치를 표현\n",
        " > $x_{new} = \\frac{x-x_{min}}{x_{max} - x_{min}}$\n",
        "\n",
        " * Noisy Data 제거 : 쓸모없는 데이터를 제거하는 것 또한 중요하다.\n",
        "학습을 진행할 때 방해가 되는 데이터는 모두 `Noisy data`이다\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tic7yiZR-wrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}