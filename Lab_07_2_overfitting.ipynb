{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-07-2-overfitting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKYWx8vU3vuoMwWOY+3fzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow/blob/master/study-note/Lab_07_2_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MIFxjbximaY",
        "colab_type": "text"
      },
      "source": [
        "#Overfitting\n",
        "\n",
        "* ML상에서 흔히 접할 수 있는 경우이다.\n",
        "* over(과하게) + fitting(맞춰져있다.)\n",
        "* 모델은 hypothesis를 맞춰가면서 점점 `Accuracy`가 증가한다.\n",
        "* 우리가 학습에 사용된 데이터를 이용하여 `Accuracy`를 측정하므로\n",
        "학습에 사용되지 않은 `새로운 데이터`를 이용하여 test를 하면 정확도가 떨어지는모델이 나오는 경우가 있다. <br />\n",
        "이것이 바로 `Overfitting`!! \n",
        "* 우리의 이상적인 모델은  평가데이터, 테스트데이터 무엇으로 하던, 높은 `Accuracy`가 나와야한다.\n",
        "\n",
        "> 참고.  `underfit` <br />\n",
        "학습이 덜 된 상태.\n",
        "\n",
        "> 참고.\n",
        "`overfitting` - High variance\n",
        "`underfitting` - High bias\n",
        "\n",
        "## 어떻게 해결할 수 있을까?\n",
        " * feature를 어떻게 정하느냐에따라 해겨할 수 있는 다양한 방법이 존재.\n",
        "  \n",
        "  1. 더 많은 데이터를 트레이닝 시킨다. (`Data Augmentation`)\n",
        "  > overfitting된 것을 좀 더 general하게(보편적이게, 데이터종속적이지 않게) 희석시키는 개념이다.\n",
        "  > * 이미지 데이터의 경우, 아래와 같은 방법들이 존재.\n",
        "  >> * `Color Jittering` - 다양하게 색상제공\n",
        "  >> * `Horizontal Flips` - 색반전.\n",
        "  >> * `Random Crops/Scales` - 크기 조정.\n",
        "\n",
        "  2. feature의 수를 줄인다.\n",
        "  > `dataset`의 `dimension`을 줄인다는 것이다.\n",
        "\n",
        "  3. feature의 수를 늘린다. \n",
        "  > 모델이 데이터를 제대로 설명하지 못하는 경우에 쓸 수 있는 방법이다. =`underfitting`<br />\n",
        "  모델을 좀 더 구체화 시키기위해 학습에 의미있는 feature를 늘린다.\n",
        "\n",
        "  4. Regularization\n",
        "  > * Normalization`과 `Standardization`과 다른 개념이다! <br />\n",
        "  > * Overfitting은 모델이 train data에 너무 딱 맞게 학습이 돼서 발생하는 문제인데 여기서 특정 penalty 값만 더해주거나 빼주어서 모델의 복잡도를 조정<br />\n",
        "  > * `hyper-parameter`를 수정하는 방식으로 `regularization`을 수행.\n",
        "  > * `L1 regularization`, `L2 regularization`방법이 있다.\n",
        "\n",
        "\n",
        "## 그 외에도 Dropout과 Batch Normalization방법이 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvboVTjJiMt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d98e5ad2-84a4-4179-f710-a130b35612f8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I4CdUSniQC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}