{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_10: NN, ReLu, Xavier, Dropout, Optimizer like Adam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8nPuQpZjt1t8y64I+EmdB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow-study-note/blob/master/Lab_10_NN%2C_ReLu%2C_Xavier%2C_Dropout%2C_Optimizer_like_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqP9EgXmJcCj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Lab_10: NN, ReLu, Xavier, Dropout, Optimizer like Adam\n",
        "\n",
        "[동영상 강의](https://www.youtube.com/watch?v=6CCXyfvubvY&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=34)\n",
        " * network 구성에 따른 비교가 잘 되어있음.\n",
        "\n",
        "# 1. NN (Neural Network)\n",
        "* 하나 이상의 `layer`로 구성된 node들의 network.\n",
        "* \n",
        " #### Feedforward Network\n",
        " * 순방향 신경망은 노드 간의 연결이 `순환`을 형성하지 \"않는\" 인공 신경망이다. 즉, 이는 순환 신경망과 차이가 있다. \n",
        " * 순방향 신경망은 고안된 인공 신경망의 최초의 가장 단순한 형태.\n",
        "\n",
        " #### Split & Merge\n",
        "  * layer가 넘어가면서 노드가 두갈래로 나뉘는 것.\n",
        "  * 두 갈래로 나뉘었던 노드가 하나의 노드 input으로 들어가는것.\n",
        "  * 각각을 학습하고 뒷 layer에서 모이는 형태 - `ConvNet`\n",
        "\n",
        " #### Recurrent Network\n",
        "  * `RNN`\n",
        "\n",
        "\n",
        " ### 결론은 Network는 어떤 형태로든지 구성이 가능하다는 것이다.\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "# 2. Initialize weights in a smart way\n",
        "\n",
        " * `backpropagation`이 `deep network`에서 제대로 작동하지 않는 이유중하나가\n",
        " > initial weight 값을 적절한 값이 아닌 랜덤한 값을 주었기 때문이다.\n",
        " * `Weight Initialize`를 잘하면 epoch초기부터 loss값이 낮게 나온다.\n",
        "  >이유는 당연히 초기값이 잘잡혔기 때문! \n",
        "\n",
        " #### RBM\n",
        "  * weight 값을 초기화하는 방법.\n",
        "  * 지금은 잘 사용되지 않는다.\n",
        "  * 매우 복잡한 방식.\n",
        "\n",
        " #### Xavier\n",
        "  * input, ouput갯수에 비례하여 weight값을 주는 방식. \n",
        " #### He \n",
        "  * Xavier에 0.5를 곱한값을 weight로 준다.\n",
        "\n",
        " ##등등... 여러가지 많은 초기화 방법이 있다.\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        " # 3. Dropout 과 앙상블\n",
        "* \n",
        " #### Dropout\n",
        "  * `overfitting`의 하나의 해결책.\n",
        "  * `neural network`의 노드의 일부연결을 끊는 것.\n",
        "  * 몇개의 노드를 죽이자. 랜덤하게.\n",
        "  * 랜덤하게 training중에 노드를 제외시키고(쉬게하고) 트레이닝을 시키는 것이다.\n",
        "  > * tensorflow에서는 dropout layer를 넣어줌으로써 쉽게 사용할 수있다.\n",
        "   * 얼마의 node를 유지할것인가, 얼마나 dropout시킬것인가 지정할 수 있다.\n",
        "   * 보통 학습시 0.5~0.7을 유지시킨다.\n",
        "\n",
        "  ### 주의 할 점\n",
        "   > training중에만 `dropout`을 시키고 <br />\n",
        " 실제 output값을 예측할때, validation 이나 test를 할 때는 <br />\n",
        " 모든 node를 사용해야한다.\n",
        "\n",
        " #### Ensemble\n",
        "  * 같은 데이터에 대하여 여러 모델을 만든후 마지막에 합쳐서 output을 낸다.\n",
        "  * 학습시킬 데이터가 많을 경우 유용할 수 있다.\n",
        "  * 2~5% 정확도 향상 효과가 있다.\n",
        "\n",
        "  # 4. Optimizer\n",
        "  * 여러가지 optimizer들이 있다.\n",
        "  * `Adam`이 일반적인 경우 좋은 효율을 낸다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC2zBo9sJZUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}