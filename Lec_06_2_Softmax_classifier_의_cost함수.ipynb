{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec_06-2: Softmax classifier 의 cost함수.ipynb의 사본",
      "provenance": [],
      "authorship_tag": "ABX9TyO3gLiGFuTH3bmkOkNunJFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow/blob/master/study-note/Lec_06_2_Softmax_classifier_%EC%9D%98_cost%ED%95%A8%EC%88%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZNAcvA4y66l",
        "colab_type": "text"
      },
      "source": [
        "# Lec_06-2: Softmax classifier 의 cost함수\n",
        "\n",
        "## softmax란?\n",
        "* ouput값이 0이상 1이하가 되게하는 classifier.\n",
        "* 모든 output의 값을 더하면 1이되도록. ** 확률로 나타낼 수 있게 한다. ** <br />\n",
        "\n",
        "> **one-hot encoding이란?** <br />\n",
        "데이터값의 갯수만큼의 벡터에서 원하는 하나의 값에만 1을 부여하고 <br />\n",
        "나머지에는 0을 부여하는 방식. <br />\n",
        "\n",
        "#### Cross Entropy란?\n",
        "* 실제값과 예측값의 차이를 구하는 함수이다. <br />\n",
        "* `Log Loss`라고 불리기도 한다.\n",
        "* `Cross-Entropy`는 Element-wise연산을 하기 때문에 `Binary classification`이 아닌 `multinomial-classification`에도 사용할 수 있다. 우리가 이전 `Logistic Regression`강의에서 배운 `cost function`이 사실은 `cross-entropy`의 `binary classification`인 경우였던것이다!.\n",
        "> Cross-entropy loss is split into two separate cost functions when dealing with a binary classification problem: for y=0 and y=1.\n",
        "\n",
        "<br />\n",
        "이전의 강의에서 배웠던 <br />\n",
        "`Logistic cost function`과 `Cross-Entropy cost function`은  근본적으로 같은것이다.\n",
        "\n",
        "### 결론 \n",
        "어쨋든 `softmax`는 0이상1이하 확률로 output이나오는 Classifier(Hypothesis)이고 <br /> \n",
        "cost function으로는 `Cross-Entropy`를 사용할 수 있다.<br />\n",
        "그 이후에는 cost를 구했으니 `optimization`과정이 필요한 것이다.\n",
        "`Gradient Descent`를 하라는 것이다.\n",
        "요약하자면 [softmax] - [cross_entropy] - [gradient descent]할 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UsHd7THxrC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}