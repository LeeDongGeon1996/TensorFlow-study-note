{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec_10_4 Batch Normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJNTMSax3r0RiM5UCjHpdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow-study-note/blob/master/Lec_10_4_Batch_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dwhfky9gxUn",
        "colab_type": "text"
      },
      "source": [
        "# Lec_10_4 Batch Normalization\n",
        "\n",
        "[부가 설명 엄청 잘돼있음.](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)\n",
        "\n",
        "## Batch Normalization\n",
        " * input의 분포가 학습과정에서 틀어지는 경우가 있다.(무슨말이지..데이터가 편향적으로 들어간다는 말인가?)\n",
        " *  Internal Covariate Shift를 방지하기 위해 사용하는 방법.\n",
        " > Batch Normalization은 <br />\n",
        "  ```\n",
        "   tf.keras.layers.BatchNormalization()\n",
        "  ```\n",
        "  으로 사용가능<br />\n",
        "\n",
        "\n",
        "  * `layer` - `norm` - `activation` 으로 구성하는 경우가 있고 \n",
        "  * `norm` - `activation` - `layer` 으로 구성하는 경우가 있다.\n",
        "\n",
        " > 이 때, dropout의 위치는 ??? 어디가 적합할까..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI3AiDwrgv9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}