{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec_12_1_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkvjEr+/O+zspBg+fkm+cq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow-study-note/blob/master/Lec_12_1_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7q33X3O8a5U",
        "colab_type": "text"
      },
      "source": [
        "# Lec_12_1_RNN\n",
        "\n",
        "## Concept\n",
        " * sequence data\n",
        "  > 데이터의 순서가 하나의 중요한 요소인 데이터 집합.\n",
        "\n",
        " * RNN의 한계\n",
        "  * RNN 역시 layer가 깊어지면 학습하는데에 어려움이 있다.\n",
        "   > * LSTM - Long Short Term Memory\n",
        "   > * GRU\n",
        "\n",
        " * RNN의 다양한 구현\n",
        "  * one to many : image captioning\n",
        "  * many to one : sentimental classification(긍정/부정 분류)\n",
        "  * many to many : Machine translation, 형태소 분석\n",
        "\n",
        "\n",
        "## RNN: many to one\n",
        " * 어떤 sentence가 input으로 주어지고 polarity(긍/부정)을 판단할 때.\n",
        " * sentence를 word단위로 tokenization한다.\n",
        " * tokenized word는 말그대로 단어이다. 이것을 연산을 위해 숫자(vector)로 바꿔주는 embedding과정이 필요하다.\n",
        "  > * 요기서는 character별로 embedding을 진행하였다.\n",
        "  > * one-hot vector로 embedding을 진행한다.\n",
        "\n",
        " * ### Stacking\n",
        "  * CNN에서과 마찬가지로 RNN에서도 Rucurrent Neural Layer를 여러개 쌓을 수 있다.\n",
        "  > Multi-layered RNN , Stacked RNN\n",
        "\n",
        "  * RNN에서도 stacked(deeper) RNN이 shallow RNN보다 좋은 성능을 보인다. CNN layer가 쌓이는 것과 비슷한 효과.\n",
        "  > * RNN에서 input과 가까운 layer는 Syntactic information(문법), output과 가까운 layer는 Semantic information(의미)을 더 잘 encoding한다는 것이 실증됨.\n",
        "  > * The initial layers takes care of the smaller details of the image and deeper layer are able to identify the the bigger picture.\n",
        "  > * CNN에서 input과 가까운 layer는 edge와 같은 global한 feature들을 뽑아낼 수 있었다. output과 가까운 layer는 좀더 abstract한 feature들을 뽑아낸다.\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "## RNN: many to many\n",
        " * sentence를 word단위로 tokenization한다.\n",
        " * 각각의 input sequence에 대하여 output을 반환하는 모델이다.\n",
        " * 각각의 input에대하여 output이 반환되기 때문에 <br />\n",
        " 각 output마다 loss를 계산하여 평균을 낸다. -> `sequence loss`\n",
        " * 데이터의 sequence의 길이가 다를 때, batch연산을 위해 `pad`토큰을 넣는다.\n",
        "  > CNN에서 padding채워주는 것과 같은 개념.\n",
        "\n",
        " * `masking` : loss를 계산할 때, pad토큰에 대해서는 계산하지 않는 기법\n",
        " > 코드를 보니까 padding이 들어있는 index는 0(false) 데이터 index는 1(True)를 주어 <br />\n",
        " loss 행렬에 곱하는 방식으로 padding의 loss를 무효화 시키는 방식인듯하다.\n",
        "\n",
        "\n",
        "## RNN: many to many (Bidirectional)\n",
        " * many to many 방식의 모델일 때, 단방향  RNN은 정보의 불균형이 존재한다.\n",
        " > RNN모델에서 input이 순차적으로(token) 들어가게되는데, <br />\n",
        " 첫번째 토큰에대해서는 이전의 node에서 전달될 hidden state가 없다.\n",
        " 하지만 두번째 토큰은 1번째 토큰으로 부터 hidden state를 받아 2개의 정보(2번째토큰, 1번째토큰으로부터의 hidden state)를 활용하여 output을 출력할 수 있다. <br />\n",
        " >>이것은 더 후순위에 있는 토큰일수록 차이가 커지게 된다.\n",
        "\n",
        " * Bidirectional RNN은 이러한 문제를 해결하기 위해 다음 두 가지를 활용한다.\n",
        "  * Forward RNN : sequence를 순방향으로 input [1-2-3-4]\n",
        "  * Backward RNN : sequence를 역방향으로 input [4-3-2-1]\n",
        "\n",
        "## RNN: sequence to sequence\n",
        " * chat-bot, Machine translation, encoder-decoder format\n",
        " * Teacher forcing : 첫번째 hidden state가 오답일경우를 대비하여\n",
        " 강제로 다음 hidden state를 지정하여 완전히 다른 흐름으로 빠져버리는 걸 방지...?\n",
        " * Attention\n",
        " > sequence에서 특정 data에 집중하여(가중치를 주어) training하는 방법.\n",
        "\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "## RNN application\n",
        "  * Language model\n",
        "  * Speech recognition\n",
        "  * Machine translation\n",
        "  * Conversation modeling/Question answering\n",
        "  * Image/video captioning\n",
        "  * Image/Music/Dance generation\n",
        "   > 내가 Experiment6에서 MNIST data를 CNN만을 활용해서 새로 생성하려던 것이 불가능했었다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aONFtf5s8YZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}