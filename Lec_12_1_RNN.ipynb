{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec_12_1_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHtEppJCVLO5qIMrBRJ0ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow-study-note/blob/master/Lec_12_1_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7q33X3O8a5U",
        "colab_type": "text"
      },
      "source": [
        "# Lec_12_1_RNN\n",
        "\n",
        "## Concept\n",
        " * sequence data\n",
        "  > 데이터의 순서가 하나의 중요한 요소인 데이터 집합.\n",
        "\n",
        " * RNN의 한계\n",
        "  * RNN 역시 layer가 깊어지면 학습하는데에 어려움이 있다.\n",
        "   > * LSTM - Long Short Term Memory\n",
        "   > * GRU\n",
        "\n",
        " * RNN의 다양한 구현\n",
        "  * one to many : image captioning\n",
        "  * many to one : sentimental classification(긍정/부정 분류)\n",
        "  * many to many : Machine translation, 형태소 분석\n",
        "\n",
        "## RNN: many to one\n",
        " * 어떤 sentence가 input으로 주어지고 polarity(긍/부정)을 판단할 때.\n",
        " * sentence를 word단위로 tokenization한다.\n",
        " * tokenized word는 말그대로 단어이다. 이것을 연산을 위해 숫자(vector)로 바꿔주는 embedding과정이 필요하다.\n",
        "  > * 요기서는 character별로 embedding을 진행하였다.\n",
        "  > * one-hot vector로 embedding을 진행한다.\n",
        "\n",
        " * ### Stacking\n",
        "  * CNN에서과 마찬가지로 RNN에서도 Rucurrent Neural Layer를 여러개 쌓을 수 있다.\n",
        "  > Multi-layered RNN , Stacked RNN\n",
        "\n",
        "  * RNN에서도 stacked(deeper) RNN이 shallow RNN보다 좋은 성능을 보인다. CNN layer가 쌓이는 것과 비슷한 효과.\n",
        "  > * RNN에서 input과 가까운 layer는 Syntactic information(문법), output과 가까운 layer는 Semantic information(의미)을 더 잘 encoding한다는 것이 실증됨.\n",
        "  > * The initial layers takes care of the smaller details of the image and deeper layer are able to identify the the bigger picture.\n",
        "  > * CNN에서 input과 가까운 layer는 edge와 같은 global한 feature들을 뽑아낼 수 있었다. output과 가까운 layer는 좀더 abstract한 feature들을 뽑아낸다.\n",
        "\n",
        "\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "## RNN application\n",
        "  * Language model\n",
        "  * Speech recognition\n",
        "  * Machine translation\n",
        "  * Conversation modeling/Question answering\n",
        "  * Image/video captioning\n",
        "  * Image/Music/Dance generation\n",
        "   > 내가 Experiment6에서 MNIST data를 CNN만을 활용해서 새로 생성하려던 것이 불가능했었다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aONFtf5s8YZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}