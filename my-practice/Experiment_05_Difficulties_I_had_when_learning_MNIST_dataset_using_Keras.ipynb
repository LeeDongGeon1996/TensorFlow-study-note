{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment_05_Difficulties I had when learning MNIST dataset using Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFsNe/1nq0mV2rnbQfQSxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongGeon1996/TensorFlow-study-note/blob/master/my-practice/Experiment_05_Difficulties_I_had_when_learning_MNIST_dataset_using_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNNCVU90132i",
        "colab_type": "text"
      },
      "source": [
        "# Experiment_05_Difficulties I had when learning MNIST dataset using Keras\n",
        "\n",
        "아래와 같은 모델이 있을 때, \n",
        " * Keras의 compile, fit을 활용한 학습과\n",
        " * Tensorflow의 GradientTape을 이용한 학습\n",
        "\n",
        "으로 구현을 해보았다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADkb0trg1zyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu, padding='SAME', \n",
        "                                  input_shape=(28, 28, 1)))\n",
        "    model.add(keras.layers.MaxPool2D(padding='SAME'))\n",
        "    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n",
        "    model.add(keras.layers.MaxPool2D(padding='SAME'))\n",
        "    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n",
        "    model.add(keras.layers.MaxPool2D(padding='SAME'))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "    model.add(keras.layers.Dropout(0.4))\n",
        "    model.add(keras.layers.Dense(10))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BC5syqq2WQ_",
        "colab_type": "text"
      },
      "source": [
        " * 먼저 keras를 활용한 경우 loss function을 직접 만들어 줄 때 주의 할 점이 있었다.\n",
        "  * loss2_fn(y_true, y_pred)순으로 parameter가 입력이 되어야하며,\n",
        "  * loss function을 사용할때 \n",
        "  > * from_logits=True 이면 model을 통과한 prediction값이 확률 분포가 아닌 형태로 주어진다는 것을 알린다. 따라서 내부적으로 확률 분포 형태로 바꾸어 loss값을 계산한다.\n",
        "    *하지만 from_logits=False인 경우 확률 분포형태로 prediction값이 입력된 것으로 간주한다.\n",
        "  * 만약 model의 output layer의 activation function이 softmax라면 model의 ouput이 확률분포 형태로 나오게 되므로 False를 사용해도 된다.\n",
        "  * 하지만 위의 model과 같이 activation function이 없다면 확률 분포ouput이 아니므로 꼭 True값을 주어야한다,\n",
        "  * 다른 방법으로는 model을 수정하여 output layer의 activation function을 softmax로 주거나\n",
        "  * 또 다른 방법으로는 loss계산 하기전 prediction값을 softmax에 통과시킨후 계산한다.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSAET6p12SZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss2_fn(labels, images_soft):\n",
        "  \n",
        "    #logits = model(images, training=True)\n",
        "    #images_soft = tf.nn.softmax(images)\n",
        "\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_pred=images_soft, y_true=labels, from_logits=True, label_smoothing=0)\n",
        "    #loss = tf.keras.losses.categorical_crossentropy(y_pred=images, y_true=labels, from_logits=False)\n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss2_fn, metrics=['accuracy'])\n",
        "model.fit(x=train_dataset, epochs=15, validation_data=test_dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm1aKkvNxNMK",
        "colab_type": "text"
      },
      "source": [
        " * Tensorflow의 GradientdescentTape을 활용한 방법은 아래와 같다. \n",
        "  * 별다른 특이한 점은 없었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaS9MTn5xY1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train my model\n",
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_loss = 0.\n",
        "    avg_train_acc = 0.\n",
        "    avg_test_acc = 0.\n",
        "    train_step = 0\n",
        "    test_step = 0    \n",
        "    \n",
        "    for images, labels in train_dataset:\n",
        "        train(model, images, labels)\n",
        "        #grads = grad(model, images, labels)                \n",
        "        #optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        loss = loss_fn(model, images, labels)\n",
        "        acc = evaluate(model, images, labels)\n",
        "        avg_loss = avg_loss + loss\n",
        "        avg_train_acc = avg_train_acc + acc\n",
        "        train_step += 1\n",
        "    avg_loss = avg_loss / train_step\n",
        "    avg_train_acc = avg_train_acc / train_step\n",
        "    \n",
        "    for images, labels in test_dataset:        \n",
        "        acc = evaluate(model, images, labels)        \n",
        "        avg_test_acc = avg_test_acc + acc\n",
        "        test_step += 1    \n",
        "    avg_test_acc = avg_test_acc / test_step    \n",
        "\n",
        "    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \n",
        "          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \n",
        "          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\n",
        "    \n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}